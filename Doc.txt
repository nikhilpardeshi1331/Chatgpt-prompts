ETL Process Flow Runbook

Created by: Nithya Manivannan
Last modified by: Siddhant Shivam on Oct 04, 2024
CHG3944713


---

Introduction

This documentation outlines the ETL (Extract, Transform, Load) process developed for the HSBC project. The ETL process plays a crucial role in data integration by ensuring that data is accurately extracted from various source systems, transformed according to predefined business rules, and loaded into a target data warehouse for reporting, analysis, and decision-making. This guide is intended for data engineers, analysts, and other stakeholders involved in maintaining, updating, or troubleshooting the ETL pipeline.


---

Purpose

The purpose of this documentation is to provide a comprehensive guide to the ETL process implemented for the HSBC project. It details the steps and procedures necessary to ensure:

Data accuracy

Scalability

Compliance with HSBC's internal data governance policies and external regulations.



---

Objective

1. Data Accuracy
Ensure that data extraction from source systems maintains the integrity of the data throughout the ETL process, minimizing errors and inconsistencies.


2. Transformation Logic
Apply business rules and data transformation logic tailored to meet HSBC's specific reporting requirements, ensuring the data is formatted correctly for final usage.


3. Scalability
Design the ETL process to efficiently handle increasing data volumes as the project scales, optimizing performance at every stage.


4. Compliance
Adhere to HSBC's data governance and security standards throughout the ETL process, ensuring compliance with all relevant internal and external regulations.




---

Detailed ETL Steps

Step 1: Creating BigQuery (BQ) Scripts

Objective:
Create temporary tables to manage data before final loading into the production tables. This step ensures that data is staged and processed in an organized manner.

Action:
Implement three BQ Scripts for a Slowly Changing Dimension Type 2 (SCD2) process: TEMPI_LOAD, TEMP2_LOAD, and TEMP3_LOAD. These temporary tables are created in the RDL (Raw Data Layer) source table environment and act as staging areas for the data before further processing.


Step 2: Dataflow Pipeline for Lift and Shift

Objective:
Facilitate the movement of data from BigQuery to PostgreSQL for lift-and-shift operations.

Action:
Use a Google Dataflow pipeline to extract data from BigQuery and load it into PostgreSQL staging tables in the Cloud SQL PostgreSQL instance. This process ensures data is moved from the staging table to the final table for further reporting and analysis.

Extract data from BigQuery source tables.

Load data into PostgreSQL staging tables.

Once staging is complete, move the data from staging to final Cloud SQL PostgreSQL tables.



Step 3: Data Transformation and Validation

Objective:
Ensure that the data is transformed according to predefined business rules before loading it into production tables.

Action:
Transform data based on the business logic, apply validation checks, and cleanse the data to ensure quality and integrity.


Step 4: Loading Data into Final Tables

Objective:
Safely move the transformed and validated data into the final tables in PostgreSQL.

Action:
Perform data migration from the staging tables to the final Cloud SQL PostgreSQL tables, ensuring that all validations have been met. This step finalizes the ETL process by ensuring data is ready for analysis and reporting.



---

ETL Workflow

1. Extract Phase

Extract raw data from BigQuery.

Load it into the staging tables in PostgreSQL.



2. Transform Phase

Apply SCD2 logic for tracking historical changes.

Validate and cleanse data.

Move transformed data from staging to final production tables.



3. Load Phase

Final data load into Cloud SQL PostgreSQL tables.

Data is now ready for reporting, analysis, and business intelligence activities.

This document details the ETL (Extract, Transform, Load) process created for the HSBC project. The ETL process is essential for data integration, ensuring that data is accurately extracted from multiple source systems, transformed based on defined business rules, and then loaded into a target data warehouse for reporting, analysis, and decision-making. It is designed for data engineers, analysts, and other stakeholders responsible for maintaining, updating, or troubleshooting the ETL pipeline.









