Pega ETL Process Flow Runbook

Introduction

This document outlines the Pega ETL (Extract, Transform, Load) process created for the HSBC project. The ETL process is critical for seamless data integration, ensuring accurate extraction from various source systems, transformation based on predefined business rules, and loading into a target data warehouse for reporting, analysis, and decision-making. It is intended for data engineers, analysts, and other stakeholders responsible for managing, updating, or troubleshooting the ETL pipeline.

Purpose

The purpose of this document is to serve as a comprehensive guide to the ETL process implemented in the HSBC project. It details the procedures required to maintain data accuracy, scalability, and compliance with HSBC's internal data governance standards and external regulations.

Objective

Data Accuracy: Ensure that data extraction from source systems maintains integrity throughout the ETL process, minimizing errors and inconsistencies.

Transformation Logic: Apply business rules and data transformation logic tailored to meet HSBC's reporting requirements, ensuring proper formatting for analysis.

Scalability: Design the ETL process to handle increasing data volumes as the project scales, optimizing performance at every stage.

Compliance: Ensure the ETL process adheres to HSBC's data governance and security standards, complying with both internal and external regulations.



---

Detailed ETL Steps

Step 1: Attribute-based BigQuery Script Creation

Objective: The process begins by receiving a requirement sheet from the business team that outlines the necessary data attributes. Based on these requirements, BigQuery (BQ) insert scripts are created to manage the data extraction from BigQuery.

Action:

1. Create BQ Insert Scripts: Build BQ insert scripts based on the business requirements for attribute-based data extraction.


2. Create Update Scripts: Some tables require updates after the initial insert; create corresponding BQ update scripts to handle these changes after the records have been inserted.


3. Insert into PostgreSQL: These scripts are then used in the Dataflow pipeline to load the extracted data into PostgreSQL staging tables.




Step 2: Dataflow Pipeline for Lift-and-Shift Operations

Objective: Facilitate the movement of data from BigQuery to PostgreSQL, performing a lift-and-shift operation at the attribute level.

Action:

1. Extract from BigQuery: Data is extracted from BigQuery based on the insert scripts.


2. Load into PostgreSQL Staging Tables: Dataflow moves the data into PostgreSQL staging tables, enabling further processing.


3. Transfer from Staging to Final Table: Once the data is staged, it is transferred from the PostgreSQL staging tables to the final tables in the Cloud SQL PostgreSQL instance.




Step 3: Data Transformation and Validation

Objective: Ensure that the extracted data is transformed and validated before being loaded into the production tables.

Action:

1. Apply Business Logic: Transform data according to the predefined business rules.


2. Validate and Cleanse Data: Perform validation checks and data cleansing to ensure accuracy and consistency.




Step 4: Loading Data into Final Tables

Objective: Finalize the data load by transferring the transformed data from the staging tables to the final tables in PostgreSQL.

Action:

1. Move Data from Staging to Final Tables: Once the data is validated, it is loaded into the final Cloud SQL PostgreSQL tables.


2. Truncate Staging Tables: After the data is successfully loaded into the final tables, the staging tables are truncated to clear any temporary data.





---

ETL Workflow

1. Extract Phase:

Extract raw data from BigQuery using the BQ scripts created based on the requirement sheet.

Load the extracted data into PostgreSQL staging tables.



2. Transform Phase:

Apply business rules and transformation logic.

Validate and cleanse the data before loading it into the final PostgreSQL tables.



3. Load Phase:

Transfer the transformed data from the staging tables into the final Cloud SQL PostgreSQL tables.

Truncate staging tables after successful data load.





---

Dataflow Process Overview

The dataflow process is divided into three key stages:

1. Data Extraction and Staging:

A Dataflow job is used to extract data from BigQuery based on the insert scripts and load it into the PostgreSQL staging tables. The pipeline also interacts with a configuration table to manage the status of the load.


Configuration Table:

Contains columns such as unique_num, table_name, bq_script_path, load_type (insert/update), and status (open/closed).

The table tracks the status of the data load process, helping in the smooth execution of the pipeline.



2. Re-Open Configuration Table:

After data is successfully loaded into the final table, the configuration table is updated to mark the status as OPEN, making it ready for the next operation cycle.



3. Final Data Load:

The dataflow pipeline transfers data from the staging tables to the final PostgreSQL tables.

After the final load, the staging tables are cleared to prepare for future loads.





---

Testing and Validation

After data is loaded into PostgreSQL, thorough testing is performed to ensure that the data has been populated correctly.

Data validation checks are run to ensure that all records are accurate and comply with the business logic and transformation rules.



---

This structured approach ensures that the Pega ETL process is efficient, scalable, and aligned with HSBC's data governance and compliance standards. Let me know if you need any further adjustments or additions.

