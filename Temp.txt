from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.providers.google.cloud.sensors.gcs import GCSObjectExistenceSensor
from airflow.providers.postgres.transfers.bigquery_to_postgres import BigQueryToPostgresOperator
from datetime import datetime

# Define the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 10, 10),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
}

with DAG(
    dag_id='gcs_to_bq_load_dag',
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
) as dag:

    bucket_name = 'your-bucket-name'
    source_objects_name = 'path/to/your/csv/file.csv'
    object_ctl_file_path = 'path/to/your/ctl/file.ctl'
    destination_table = 'your_project.your_dataset.your_table'
    
    # Sensor for CSV file
    wait_for_csv_file = GCSObjectExistenceSensor(
        task_id='wait_for_csv_file',
        bucket=bucket_name,
        object=source_objects_name,
        timeout=600,
        poke_interval=30,
    )

    # Sensor for CTL file
    wait_for_ctl_file = GCSObjectExistenceSensor(
        task_id='wait_for_ctl_file',
        bucket=bucket_name,
        object=object_ctl_file_path,
        timeout=600,
        poke_interval=30,
    )

    # Task to call external Python script for GCS to BQ loading
    load_gcs_to_bq = BashOperator(
        task_id='load_gcs_to_bq',
        bash_command=f'python3 /path/to/load_gcs_to_bq.py',  # Adjust the path
    )

    # Load data into BigQuery
    load_data_pega_aar = GCSToBigQueryOperator(
        task_id='load_gcs_to_bq_operator',
        gcp_conn_id='gcp_connection',
        bucket=bucket_name,
        source_objects=source_objects_name,
        destination_project_dataset_table=destination_table,
        autodetect=True,
        source_format='CSV',
        field_delimiter=',',  # Adjust delimiter if needed
        max_bad_records=1000,
        ignore_unknown_values=True,
        write_disposition='WRITE_TRUNCATE',
    )

    # Transfer data to PostgreSQL
    transfer_to_postgres = BigQueryToPostgresOperator(
        task_id='transfer_bq_to_pg',
        gcp_conn_id='gcp_connection',
        dataset_table=f'{destination_table}',
        postgres_conn_id='postgres_connection_id',
        target_table_name='your_postgres_table',
        replace=False,
    )

    # Task dependencies
    wait_for_csv_file >> wait_for_ctl_file >> load_gcs_to_bq >> load_data_pega_aar >> transfer_to_postgres
