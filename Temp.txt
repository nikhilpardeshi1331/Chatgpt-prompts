from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from datetime import datetime

# Define default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'retries': 1
}

# Define the DAG
dag = DAG(
    dag_id='NS_PEGA_PSQL',
    default_args=default_args,
    catchup=False,
    schedule_interval=None
)

# Task 1: Create a temporary directory (this step is optional since we are using GCS for storage)
PEGA_ETL_SRC_job3 = BashOperator(
    task_id='PEGA_ETL_SRC_job3',
    bash_command='mkdir -p /tmp/ns_pega_jobs',
    dag=dag,
)

# Task 2: Execute the psql command and store the output into a temporary file, then upload to GCS
PEGA_ETL_SRC_job4 = BashOperator(
    task_id='PEGA_ETL_SRC_job4',
    bash_command=(
        'psql -h 192.168.192.46 -U uat-sql-auth-proxy@hsbc-12453728-pegacdhhk-dev.iam -d dhkpr00023 -p 5432 '
        '-c "select account_id from isshktal.aar_ns_sit limit 5" > /tmp/ns_logs.txt && '
        'gsutil cp /tmp/ns_logs.txt gs://your-bucket/ns_pega_jobs/ns_logs.txt'
    ),
    dag=dag,
)

# Task 3: Download and print the contents of the log file from GCS
PEGA_ETL_SRC_job5 = BashOperator(
    task_id='PEGA_ETL_SRC_job5',
    bash_command='gsutil cat gs://your-bucket/ns_pega_jobs/ns_logs.txt',
    dag=dag,
)

# Define task dependencies
PEGA_ETL_SRC_job3 >> PEGA_ETL_SRC_job4 >> PEGA_ETL_SRC_job5
