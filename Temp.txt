from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.providers.google.cloud.operators.gcs import GCSListObjectsOperator
from datetime import datetime
import os

# Configuration
GCS_BUCKET_NAME = "your-gcs-bucket-name"
BIGQUERY_DATASET = "your_bigquery_dataset"

# Default arguments for the DAG
default_args = {
    "start_date": datetime(2025, 1, 1),
    "retries": 1,
}

# Define the DAG
with DAG(
    "load_csv_to_bigquery_dynamically",
    default_args=default_args,
    schedule_interval=None,  # Trigger manually or as needed
    catchup=False,
) as dag:

    # Step 1: List all CSV files in the GCS bucket
    list_csv_files = GCSListObjectsOperator(
        task_id="list_csv_files",
        bucket=GCS_BUCKET_NAME,
        prefix="",  # Set a folder prefix if needed, e.g., "csv-folder/"
    )

    # Step 2: Define a function to generate tasks dynamically
    def create_bigquery_load_task(file_name):
        # Derive the table name from the file name
        table_name = os.path.splitext(os.path.basename(file_name))[0]
        table_id = f"{BIGQUERY_DATASET}.{table_name}"
        gcs_uri = f"gs://{GCS_BUCKET_NAME}/{file_name}"

        return BigQueryInsertJobOperator(
            task_id=f"load_{table_name}_to_bigquery",
            configuration={
                "load": {
                    "sourceUris": [gcs_uri],
                    "destinationTable": {
                        "projectId": "{{ var.value.gcp_project }}",  # Set your GCP project
                        "datasetId": BIGQUERY_DATASET,
                        "tableId": table_name,
                    },
                    "sourceFormat": "CSV",
                    "skipLeadingRows": 1,  # Skip header row
                    "autodetect": True,  # Automatically detect schema
                }
            },
            location="US",  # Adjust based on your BigQuery dataset location
        )

    # Step 3: Dynamically create load tasks based on the listed files
    from airflow.models import TaskInstance

    def create_dynamic_tasks(**context):
        ti = context["ti"]
        files = ti.xcom_pull(task_ids="list_csv_files")
        if not files:
            raise ValueError("No CSV files found in the bucket.")
        load_tasks = []
        for file in files:
            if file.endswith(".csv"):
                load_tasks.append(create_bigquery_load_task(file))
        return load_tasks

    # Step 4: Dynamic task generation
    dynamic_load_tasks = create_dynamic_tasks

    # DAG Workflow
    list_csv_files >> dynamic_load_tasks
