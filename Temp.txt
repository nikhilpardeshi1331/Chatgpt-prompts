import configparser
import io
import logging
from google.cloud import bigquery, storage

def load_config(bucket_name, file_path):
    """Load configuration from GCS."""
    client = storage.Client()
    bucket_obj = client.bucket(bucket_name)
    blob_obj = bucket_obj.blob(file_path)
    config_content = blob_obj.download_as_text()
    
    config = configparser.ConfigParser()
    config.read_file(io.StringIO(config_content))
    logging.info(f"Config sections: {config.sections()}")
    return config

def load_data_into_bq(config_path):
    """Load data from GCS to BigQuery."""
    config = load_config(config_path['bucket'], config_path['file_path'])
    
    project_id = config.get('GSC_BQ_LOAD', 'project_id')
    dataset_id = config.get('GSC_BQ_LOAD', 'dataset_id')
    table_id = config.get('GSC_BQ_LOAD', 'table_id')
    file_path = config.get('GSC_BQ_LOAD', 'file_path')

    # GCS to BQ
    client = bigquery.Client()
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        autodetect=True,
        skip_leading_rows=1,
    )
    
    uri = f"gs://{config_path['bucket']}/{file_path}"
    load_job = client.load_table_from_uri(uri, f"{project_id}.{dataset_id}.{table_id}", job_config=job_config)
    load_job.result()  # Wait for job to complete

    table = client.get_table(f"{project_id}.{dataset_id}.{table_id}")
    logging.info(f"Loaded {table.num_rows} rows into {table_id}")

if __name__ == "__main__":
    bucket_name = 'my-bucket'
    config_path = {
        'bucket': bucket_name,
        'file_path': 'path-to-config-file.cfg',
    }
    load_data_into_bq(config_path)
